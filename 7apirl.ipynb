{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa626e8-0f11-4049-a52f-34bcc1b8ac41",
   "metadata": {},
   "source": [
    "\n",
    "Q1. Polynomial functions and kernel functions are closely related in machine learning algorithms, particularly in support vector machines (SVMs). A polynomial kernel function is a type of kernel function used in SVMs to implicitly map input data into a higher-dimensional feature space, where linear separation may be possible. Polynomial functions, on the other hand, are mathematical functions that involve terms raised to powers, such as \n",
    "�\n",
    "2\n",
    "x \n",
    "2\n",
    " , \n",
    "�\n",
    "3\n",
    "x \n",
    "3\n",
    " , etc. In SVMs, the polynomial kernel function computes the dot product between two data points in the original feature space, raising it to a specified power \n",
    "�\n",
    "d, and adding an optional coefficient term and constant term. This allows the SVM to learn non-linear decision boundaries in the original feature space without explicitly transforming the data into higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491298d-b060-480a-9dbf-e67c661dca4a",
   "metadata": {},
   "source": [
    "Q2. To implement an SVM with a polynomial kernel in Python using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5348ec78-876e-4d11-b37e-7c0e0cdabc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess data (e.g., scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of SVC with polynomial kernel\n",
    "svc = SVC(kernel='poly')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier (e.g., accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed38d20-06ef-4c78-b851-c6b46de307a7",
   "metadata": {},
   "source": [
    "Q3. Increasing the value of epsilon in Support Vector Regression (SVR) typically leads to fewer support vectors\n",
    ". Epsilon determines the width of the margin in SVR, which affects the trade-off between fitting the training data closely and allowing more flexibility in the margin. When epsilon is increased, the margin becomes wider, allowing more training points to fall within the margin without affecting the loss function. Consequently, fewer support vectors are needed to define the margin, leading to a sparser model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b7289-938d-444f-90c5-4f0a7721fa28",
   "metadata": {},
   "source": [
    "Q4. In Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter can significantly affect the performance of the model:\n",
    "\n",
    "Kernel Function: The choice of kernel function determines the form of the decision boundary and the model's ability to capture non-linear relationships in the data. For example, a polynomial kernel may be suitable for data with polynomial relationships, while an RBF (Radial Basis Function) kernel can capture more complex, non-linear patterns.\n",
    "\n",
    "C Parameter: The C parameter controls the trade-off between the model's complexity and the degree to which deviations larger than epsilon are tolerated. A higher value of C allows for smaller margins and fewer violations of the margin constraints, potentially leading to overfitting, while a lower value of C encourages a wider margin and a more robust model.\n",
    "\n",
    "Epsilon Parameter: The epsilon parameter determines the margin of tolerance around the regression line. Increasing epsilon allows for larger deviations from the regression line without being penalized, resulting in a wider margin and potentially fewer support vectors.\n",
    "\n",
    "Gamma Parameter: The gamma parameter affects the influence of individual training samples on the decision boundary. A low value of gamma implies a large influence, leading to a smooth decision boundary, while a high value of gamma implies a smaller influence, resulting in a more complex decision boundary that may be prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742dd4c7-ba2f-4d2c-b5bd-dcb08cf93143",
   "metadata": {},
   "source": [
    " Below is the assignment completed using Python's Scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86004f33-d9e2-4ea2-9911-8e55baa62c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess data (e.g., scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of SVC with polynomial kernel\n",
    "svc = SVC(kernel='poly')\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'degree': [2, 3, 4]}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc = SVC(kernel='poly', **best_params)\n",
    "tuned_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc, 'tuned_svc_model.pkl')\n",
    "\n",
    "# Use the tuned classifier to predict labels of the testing data\n",
    "y_pred = tuned_svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier (e.g., accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d4e42-655f-4293-bc0e-48741eb6c390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
